#include <iostream>
#include <vector>
#include <memory>
// #include <NvInfer.h>
// #include <cuda_runtime_api.h>

// TODO: Implement CHECK_CUDA macro

int main() {
    std::cout << "Inference Execution Exercise" << std::endl;

    // TODO: Load Engine (Reuse Ex04) and Create Context

    // TODO: Define Sizes (input, output)

    // TODO: Allocate Host Memory

    // TODO: Allocate Device Memory (cudaMalloc)

    // TODO: Create CUDA Stream

    // TODO: Set up bindings array

    // TODO: Step 1: Copy Host -> Device (Async)

    // TODO: Step 2: Enqueue Inference (Async)

    // TODO: Step 3: Copy Device -> Host (Async)

    // TODO: Step 4: Synchronize Stream

    // TODO: Process/Print Output

    // TODO: Cleanup (cudaFree, cudaStreamDestroy)

    return 0;
}
